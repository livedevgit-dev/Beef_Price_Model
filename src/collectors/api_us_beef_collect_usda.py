import os
import requests
import pandas as pd
import time
import urllib3
from datetime import datetime, timedelta
from dotenv import load_dotenv

# [íŒŒì¼ ì •ì˜ì„œ]
# - íŒŒì¼ëª…: src/collectors/api_us_beef_collect_usda.py
# - ì—­í• : ìˆ˜ì§‘ (ë¯¸êµ­ ì†Œê³ ê¸° ì „ì²´ ì‹œì¥ ë°ì´í„°)
# - ë²”ìœ„: Choice(ìƒê¸‰), Select(ì¼ë°˜/ì €ê°€), Ground Beef(ë‹¤ì§ìœ¡), Trimmings(ìíˆ¬ë¦¬)
# - ì €ì¥: data/0_raw/usda_beef_history.csv

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
load_dotenv()

def get_api_key():
    return os.getenv("USDA_API_KEY")

def get_paths():
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    save_dir = os.path.join(base_dir, 'data', '0_raw')
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = os.path.join(save_dir, 'usda_beef_history.csv')
    return save_path

def get_last_update_date(save_path):
    if os.path.exists(save_path):
        try:
            df = pd.read_csv(save_path)
            if not df.empty and 'report_date' in df.columns:
                df['dt'] = pd.to_datetime(df['report_date'])
                last_date = df['dt'].max()
                print(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë°œê²¬: ë§ˆì§€ë§‰ ìˆ˜ì§‘ì¼ {last_date.strftime('%Y-%m-%d')}")
                return last_date
        except Exception:
            pass
    
    print("âœ¨ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: 2019-01-01ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    return datetime(2018, 12, 31)

def generate_new_dates(last_date):
    start_date = last_date + timedelta(days=1)
    end_date = datetime.now()
    if start_date > end_date:
        return []
    dates = pd.date_range(start=start_date, end=end_date, freq='B')
    date_strings = [d.strftime('%m/%d/%Y') for d in dates]
    date_strings.sort(reverse=True)
    return date_strings

def fetch_and_append():
    api_key = get_api_key()
    save_path = get_paths()
    
    # 1. ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ í™•ì¸
    last_date = get_last_update_date(save_path)
    target_dates = generate_new_dates(last_date)
    
    if not target_dates:
        print("âœ… ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        return

    # [í•µì‹¬ ìˆ˜ì •] ìˆ˜ì§‘ ëŒ€ìƒ ì„¹ì…˜ 4ì¢…ìœ¼ë¡œ í™•ëŒ€
    target_sections = [
        'Choice Cuts',    # ìƒê¸‰ ë¶€ë¶„ìœ¡
        'Select Cuts',    # ì¼ë°˜/ì €ê°€ ë¶€ë¶„ìœ¡ (ìš°ì‚¼ê²¹ ë“±)
        'Ground Beef',    # ë‹¤ì§ìœ¡/íŒ¨í‹°ìš©
        'Beef Trimmings'  # ìíˆ¬ë¦¬/ê°€ê³µìš©
    ]

    print(f"ğŸš€ ì¶”ê°€ ìˆ˜ì§‘ ì‹œì‘: {target_dates[-1]} ~ {target_dates[0]} (ì´ {len(target_dates)}ì¼)")
    print(f"ğŸ¯ ìˆ˜ì§‘ ì„¹ì…˜: {target_sections}")
    
    new_data = []
    
    # 2. ë‚ ì§œë³„ & ì„¹ì…˜ë³„ ë°ì´í„° ìš”ì²­
    for i, date_str in enumerate(target_dates):
        # ì§„í–‰ë¥  í‘œì‹œ (ì¤„ë°”ê¿ˆ ì—†ì´ ê°±ì‹ )
        print(f"\râ³ [{i+1}/{len(target_dates)}] {date_str} ë°ì´í„° 4ì¢… ìš”ì²­ ì¤‘...", end="")
        
        for section in target_sections:
            base_url = f"https://mpr.datamart.ams.usda.gov/services/v1.1/reports/2453/{section}"
            query = f"report_date={date_str}"
            
            try:
                response = requests.get(
                    base_url, 
                    auth=(api_key, '') if api_key else None, 
                    params={'q': query}, 
                    verify=False, timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, dict):
                        results = data.get('results', [])
                        if results:
                            # [ë°ì´í„° íƒœê¹…] ì–´ë–¤ ì„¹ì…˜ì—ì„œ ì˜¨ ë°ì´í„°ì¸ì§€ í‘œê¸°
                            # section_type ì»¬ëŸ¼ì— 'Choice', 'Select', 'Ground', 'Trimmings' ì €ì¥
                            clean_name = section.replace(' Cuts', '').replace('Beef ', '')
                            for item in results:
                                item['grade'] = clean_name # grade ë˜ëŠ” category ì»¬ëŸ¼ìœ¼ë¡œ í™œìš©
                            
                            new_data.extend(results)
                else:
                    time.sleep(0.2)
                    
            except Exception:
                pass
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ)
            # time.sleep(0.05)

    # 3. ë°ì´í„° ì €ì¥
    if new_data:
        df_new = pd.DataFrame(new_data)
        
        if os.path.exists(save_path):
            df_old = pd.read_csv(save_path)
            df_final = pd.concat([df_old, df_new], ignore_index=True)
        else:
            df_final = df_new
            
        # ì¤‘ë³µ ì œê±° (ë‚ ì§œ + í’ˆëª©ëª… + ë“±ê¸‰ ê¸°ì¤€)
        if 'item_description' in df_final.columns:
            subset_cols = ['report_date', 'item_description', 'grade']
        else:
            # Ground Beef/TrimmingsëŠ” item_descriptionì´ ì—†ì„ ìˆ˜ë„ ìˆìŒ (ë³´í†µ report_titleì´ë‚˜ ë‹¤ë¥¸ ê±¸ë¡œ êµ¬ë¶„)
            # ì•ˆì „í•˜ê²Œ ì „ì²´ ì¤‘ë³µ ì œê±° ì‹œë„
            subset_cols = None

        if subset_cols:
             df_final.drop_duplicates(subset=subset_cols, inplace=True, keep='last')
        else:
             df_final.drop_duplicates(inplace=True)
        
        # ì •ë ¬ (ë‚ ì§œ -> ë“±ê¸‰ -> í’ˆëª©ëª…)
        df_final['temp_dt'] = pd.to_datetime(df_final['report_date'])
        
        # ì •ë ¬ ê¸°ì¤€ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì •ë ¬
        sort_cols = ['temp_dt', 'grade']
        if 'item_description' in df_final.columns:
            sort_cols.append('item_description')
            
        df_final = df_final.sort_values(by=sort_cols, ascending=[False, True, True])
        df_final = df_final.drop(columns=['temp_dt'])
        
        df_final.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"\n\nğŸ’¾ ì—…ë°ì´íŠ¸ ì™„ë£Œ! {len(new_data)}ê±´ ì¶”ê°€ë¨ (ì´ {len(df_final)}ê±´)")
        
        # [ê²€ì¦ìš© ì¶œë ¥]
        print("\nğŸ” [ìˆ˜ì§‘ëœ ì„¹ì…˜ë³„ ê±´ìˆ˜]")
        print(df_new['grade'].value_counts())
        
    else:
        print("\nâš ï¸ ìš”ì²­í•œ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    fetch_and_append()